{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution3D, ZeroPadding3D, MaxPooling3D, Flatten, Dense, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "meta_data = './dataset/metadata/participant_d040722.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Woody\\AppData\\Local\\Temp\\ipykernel_512\\2981791683.py:1: DtypeWarning: Columns (239,240,348) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  metadata = pd.read_csv(meta_data)\n"
     ]
    }
   ],
   "source": [
    "metadata = pd.read_csv(meta_data)\n",
    "metadata = metadata.query('rndgroup == 1 ')\n",
    "\n",
    "#metadata\n",
    "metadata = metadata[[ 'pid','lung_cancer', 'conflc', 'canc_rpt_link', 'cancyr', 'can_scr', 'last_screen_studyyr' ]]\n",
    "\n",
    "def get_labels(pids, metadata_df):\n",
    "    labels = metadata_df[metadata_df['pid'].isin(pids)]\n",
    "    return pids, labels\n",
    "\n",
    "cancerous = metadata.query('lung_cancer == 1 and canc_rpt_link == 1 and can_scr == 1 and cancyr == 0')\n",
    "non_cancerous = metadata.query('lung_cancer == 0 and last_screen_studyyr == 0 ' )\n",
    "\n",
    "test_positive = cancerous.iloc[170:]\n",
    "train_positive = cancerous.iloc[0:170]\n",
    "train_sample  = train_positive.sample(n = 110)\n",
    "\n",
    "#return comma separated pids\n",
    "def cs_pid(dataframe):\n",
    "\n",
    "    comma_seperated_pids = ''\n",
    "    for i, row in dataframe.iterrows():\n",
    "        pid = row['pid'].astype(int)\n",
    "        comma_seperated_pids += f' ,{pid}'\n",
    "    print(comma_seperated_pids)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ,106001 ,108703 ,118725 ,124851 ,118192 ,202380 ,124952 ,130034 ,201264 ,116830 ,130824 ,116447 ,129973 ,103819 ,203427 ,126600 ,128343 ,126548 ,121706 ,206169 ,112313 ,209432 ,128052 ,210100 ,120354 ,203099 ,210698 ,102527 ,111877 ,210165 ,108789 ,128319 ,203563 ,117858 ,203333 ,114565 ,127682 ,206451 ,101746 ,112169 ,109454 ,102280 ,210964 ,103576 ,201852 ,130329 ,118021 ,206700 ,119929 ,109371 ,129179 ,109364 ,109324 ,105906 ,103146 ,115803 ,203491 ,111284 ,103151 ,121369 ,202982 ,103055 ,116625 ,102974 ,100766 ,128269 ,102023 ,122812 ,100568 ,110805 ,108754 ,120551 ,206672 ,108024 ,105764 ,117408 ,208527 ,132607 ,131536 ,103932 ,125826 ,204623 ,205620 ,209661 ,134164 ,100715 ,207666 ,109245 ,130193 ,121331 ,112766 ,203771 ,202733 ,128981 ,115411 ,125237 ,207325 ,120724 ,110681 ,133409 ,207877 ,126695 ,109510 ,201565 ,210745 ,209106 ,209979 ,206599 ,129633 ,118936 ,201139 ,128214 ,108900 ,130585 ,210210 ,126511 ,134526 ,111239 ,131970 ,209254 ,103764 ,127442 ,122663 ,202339 ,108820 ,127794 ,116391 ,116473 ,100671 ,121212 ,124247 ,132117 ,122270 ,209153 ,117049 ,208223 ,202276 ,119997 ,202328 ,111794 ,102303 ,204002 ,119501 ,100168 ,206653 ,200473 ,108087 ,201559 ,207440 ,117688 ,117118 ,117579 ,205532 ,201327 ,104953 ,126964 ,105507 ,124937 ,114584 ,124145 ,119161 ,118134 ,112755 ,122461 ,208610 ,202021 ,200915 ,101801 ,121660 ,203636 ,114063 ,102343 ,206302 ,113648 ,131743 ,132688 ,117734 ,104069 ,129229 ,202158 ,204012 ,202167 ,117958 ,201021 ,208285 ,117826 ,120697 ,201192 ,100849 ,132327 ,133379 ,104698 ,117886 ,121075 ,122500 ,129238 ,208488 ,123562 ,207168 ,134304\n"
     ]
    }
   ],
   "source": [
    "test_negetive = non_cancerous.iloc[600:]\n",
    "train_negetive = non_cancerous.iloc[0:800]\n",
    "train_sample  = train_negetive.sample(n = 200)\n",
    "\n",
    "#return comma separated pids\n",
    "cs_pid(train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "def return_scan_path(path, sliceThreshold):\n",
    "    scans_path = []\n",
    "    for d in os.listdir(path):\n",
    "        if os.path.isdir(f'{path}\\{d}') :\n",
    "            for dd in os.listdir(f'{path}\\{d}'):\n",
    "                if os.path.isdir(f'{path}\\{d}\\{dd}') :\n",
    "                    for ddd in os.listdir(f'{path}\\{d}\\{dd}'):\n",
    "                        if len(os.listdir(f'{path}\\{d}\\{dd}\\{ddd}')) >= sliceThreshold: \n",
    "                            scans_path.append(f'{path}\\{d}\\{dd}\\{ddd}')\n",
    "\n",
    "    return scans_path\n",
    "#return_scan_path(r'c:\\Users\\Woody\\Desktop\\lung cancer\\dataset\\train_positive\\manifest-1681052108196\\NLST', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "batch_size = 10\n",
    "positives = np.array(os.listdir(f'c:/Users/Woody/Desktop/lung cancer/dataset/train_positive/manifest-1681052108196/segmented/'))\n",
    "negetives = np.array(os.listdir(f'c:/Users/Woody/Desktop/lung cancer/dataset/train_negetive/manifest-1681053813358/segemented/'))\n",
    "data = np.concatenate((positives, negetives))\n",
    "\n",
    "\n",
    "def get_labels_from_fnames(names, metadata_df):\n",
    "    pids = np.array([int(name[:-3]) for name in names ])\n",
    "    labels = metadata_df[metadata_df['pid'].isin(pids)]\n",
    "\n",
    "    reindexed_labels = labels.set_index('pid')\n",
    "    reindexed_labels = reindexed_labels.reindex(pids)['lung_cancer'].values\n",
    "    return pids, reindexed_labels\n",
    "\n",
    "x, y = get_labels_from_fnames(data, metadata)\n",
    "\n",
    "# test size should be changed \n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.9, random_state=42)\n",
    "\n",
    "\n",
    "def get_labels(pids, metadata_df):\n",
    "    labels = metadata_df[metadata_df['pid'].isin(pids)]\n",
    "    return pids, labels\n",
    "\n",
    "\n",
    "\n",
    "class lungs_dataset(Dataset):\n",
    "    def __init__(self, x, y , path ):\n",
    "        self.x = x \n",
    "        self.y = y \n",
    "        self.path = path\n",
    "        \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pid = self.x[idx]\n",
    "        label = self.y[idx]\n",
    "\n",
    "        fname = f'{self.path}{pid}.pt'\n",
    "        tensor = torch.load(fname)\n",
    "\n",
    "        return tensor , label\n",
    "\n",
    "path_to_sdata = f'c:/Users/Woody/Desktop/lung cancer/dataset/segmented_lungs/' \n",
    "training_data = lungs_dataset(X_train, y_train, path_to_sdata)\n",
    "#tloader = training data loader \n",
    "tloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "iter_tloader = iter(tloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 320, 410, 410])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # 3D convolutional layers\n",
    "        self.conv1 = nn.Conv3d(in_channels=1, out_channels=32, kernel_size=3)\n",
    "        #self.conv2 = nn.Conv3d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        #self.conv3 = nn.Conv3d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "        \n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool3d(kernel_size=2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        #self.fc1 = nn.Linear(in_features=128 * 6 * 6 * 6, out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=10)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pass input through 3D convolutional layers and max pooling layer\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        #x = self.pool(torch.relu(self.conv2(x)))\n",
    "        #x = self.pool(torch.relu(self.conv3(x)))\n",
    "        \n",
    "        # Flatten the output from 3D convolutional layers\n",
    "        x = x.view(-1, 128 * 6 * 6 * 6)\n",
    "        \n",
    "        # Pass flattened output through fully connected layers\n",
    "        #x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "net = Net()\n",
    "data, _ = iter_tloader.next()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:73] data. DefaultCPUAllocator: not enough memory: you tried to allocate 33878753280 bytes. Buy new RAM!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mshort()\n\u001b[1;32m----> 2\u001b[0m net(data)\n\u001b[0;32m      3\u001b[0m \u001b[39m#summary( net , (1 ,320, 410, 410) )\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(net)\n",
      "File \u001b[1;32mc:\\Users\\Woody\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[0;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[0;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "Cell \u001b[1;32mIn[52], line 26\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     25\u001b[0m     \u001b[39m# Pass input through 3D convolutional layers and max pooling layer\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(torch\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)))\n\u001b[0;32m     27\u001b[0m     \u001b[39m#x = self.pool(torch.relu(self.conv2(x)))\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     \u001b[39m#x = self.pool(torch.relu(self.conv3(x)))\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     \n\u001b[0;32m     30\u001b[0m     \u001b[39m# Flatten the output from 3D convolutional layers\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m128\u001b[39m \u001b[39m*\u001b[39m \u001b[39m6\u001b[39m \u001b[39m*\u001b[39m \u001b[39m6\u001b[39m \u001b[39m*\u001b[39m \u001b[39m6\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Woody\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[0;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[0;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[1;32mc:\\Users\\Woody\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py:572\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    569\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv3d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    570\u001b[0m                     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride, _triple(\u001b[39m0\u001b[39m),\n\u001b[0;32m    571\u001b[0m                     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 572\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv3d(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    573\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:73] data. DefaultCPUAllocator: not enough memory: you tried to allocate 33878753280 bytes. Buy new RAM!"
     ]
    }
   ],
   "source": [
    "data = data.unsqueeze(1).short()\n",
    "net(data)\n",
    "#summary( net , (1 ,320, 410, 410) )\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "train_losses, test_losses, accuracy  = [] , [] , []\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr= learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0 \n",
    "correct = 0\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i, data in enumerate(tloader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        print(outputs)\n",
    "        print(labels)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        train_losses.append(loss.item())\n",
    "        with torch.no_grad():\n",
    "            patch_correct = np.count_nonzero(np.round(outputs) == labels)\n",
    "            correct += patch_correct\n",
    "            total += batch_size\n",
    "            accuracy.append(correct/ total)\n",
    "            print(f'patch_correct is : {patch_correct} \\n output is {np.round(outputs) } \\n labes are { labels} ')\n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "        if i % 15 == 14:\n",
    "            print(f'running loss for epoch {epoch } is {running_loss / 15}')\n",
    "            running_loss = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix the dataset and model and try to run on a few scans. \n",
    "# 1. this includes running the model on GPUs \n",
    "# 2. break the lungs into smaller parts to be able to run it (first try train the model with full ct-scans)\n",
    "#clean the code, put everything into relevant functions and classes, break it into 1. preprocessing 2. dataloading 3. model and training  \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "712100c6f44c9ff0355b703b47714add24be6b979b3758ea8d0490fd160b07f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
